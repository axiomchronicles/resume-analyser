{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97961d68-2b34-4e2c-8a5f-71a21329c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3051d1-c91a-47ad-aa62-c0c97af69c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PdfReader(\"./PawanKumar_Resume.pdf\")\n",
    "page = pdf.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd6bf49-8bec-4c2d-9ad7-edc933dd4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c07e7b-3122-4f87-baaf-09ef6ff07690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atext = []\n",
    "# for text in texts:\n",
    "#     atext.append(text)\n",
    "\n",
    "# print(\"\\n\".join(atext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e4c3c0-81f0-49e6-9484-3f3d883189eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r\"\\s+\", \" \", texts).strip()\n",
    "# print(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c1a60f-b902-4d2d-a216-9123ac3011e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.split(r\"(?<=[\\.\\!\\?])\\s+\", text)\n",
    "textList = [s.strip() for s in sentences if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5cf2ff2-ee06-4262-b30c-447c6adebbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = sentences.splitlines()\n",
    "# bullets = []\n",
    "# for line in lines:\n",
    "#     stripped = line.strip()\n",
    "#     if stripped.startswith((\"-\", \"‚Ä¢\", \"*\")):\n",
    "#         bullets.append(stripped.lstrip(\"-‚Ä¢* \").strip())\n",
    "#     print(bullets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c82e5f7-abb8-4d3a-b0b5-14dbcd5cae86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mst\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "import io\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import streamlit as st\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "\n",
    "\n",
    "# ==============\n",
    "# CONFIG / CONSTANTS\n",
    "# ==============\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"ML ATS Resume Analyzer\",\n",
    "    page_icon=\"üìÑ\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "WEAK_PHRASES = [\n",
    "    \"responsible for\",\n",
    "    \"worked on\",\n",
    "    \"helped with\",\n",
    "    \"assisted with\",\n",
    "    \"participated in\",\n",
    "    \"various tasks\",\n",
    "    \"duties included\",\n",
    "    \"hard-working\",\n",
    "    \"team player\",\n",
    "    \"result-oriented\",\n",
    "    \"fast learner\",\n",
    "    \"self-motivated\",\n",
    "    \"detail-oriented\"\n",
    "]\n",
    "\n",
    "ACTION_VERBS = [\n",
    "    \"achieved\", \"analyzed\", \"built\", \"created\", \"designed\", \"developed\",\n",
    "    \"implemented\", \"led\", \"managed\", \"optimized\", \"reduced\", \"improved\",\n",
    "    \"increased\", \"delivered\", \"launched\", \"owned\", \"resolved\", \"conducted\",\n",
    "    \"orchestrated\", \"shipped\", \"enhanced\", \"automated\", \"deployed\"\n",
    "]\n",
    "\n",
    "EXPECTED_SECTIONS = [\n",
    "    \"summary\", \"objective\", \"experience\", \"work experience\", \"professional experience\",\n",
    "    \"education\", \"skills\", \"projects\", \"certifications\", \"achievements\"\n",
    "]\n",
    "\n",
    "\n",
    "# ==============\n",
    "# UTILS: FILE READING\n",
    "# ==============\n",
    "\n",
    "def read_pdf(file_bytes: bytes) -> str:\n",
    "    reader = PdfReader(io.BytesIO(file_bytes))\n",
    "    text = []\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text.append(page_text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "\n",
    "def read_docx(file_bytes: bytes) -> str:\n",
    "    file_stream = io.BytesIO(file_bytes)\n",
    "    doc = docx.Document(file_stream)\n",
    "    text = []\n",
    "    for para in doc.paragraphs:\n",
    "        text.append(para.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "\n",
    "def read_txt(file_bytes: bytes) -> str:\n",
    "    try:\n",
    "        return file_bytes.decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return file_bytes.decode(\"latin-1\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def extract_text(uploaded_file) -> str:\n",
    "    if uploaded_file is None:\n",
    "        return \"\"\n",
    "    file_bytes = uploaded_file.read()\n",
    "    name = uploaded_file.name.lower()\n",
    "\n",
    "    if name.endswith(\".pdf\"):\n",
    "        return read_pdf(file_bytes)\n",
    "    elif name.endswith(\".docx\"):\n",
    "        return read_docx(file_bytes)\n",
    "    elif name.endswith(\".txt\"):\n",
    "        return read_txt(file_bytes)\n",
    "    else:\n",
    "        # fallback: try utf-8\n",
    "        return read_txt(file_bytes)\n",
    "\n",
    "\n",
    "# ==============\n",
    "# NLP / ANALYSIS HELPERS\n",
    "# ==============\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    # Simple heuristic sentence splitter\n",
    "    sentences = re.split(r\"(?<=[\\.\\!\\?])\\s+\", text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "def extract_bullets(text: str) -> List[str]:\n",
    "    lines = text.splitlines()\n",
    "    bullets = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith((\"-\", \"‚Ä¢\", \"*\")):\n",
    "            bullets.append(stripped.lstrip(\"-‚Ä¢* \").strip())\n",
    "    return bullets\n",
    "\n",
    "\n",
    "def contains_metric(text: str) -> bool:\n",
    "    # Number, percentage or currency\n",
    "    return bool(re.search(r\"(\\d+[%]?)|(\\$\\d+)\", text))\n",
    "\n",
    "\n",
    "def starts_with_action_verb(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    first_word = text.split()[0].lower()\n",
    "    return first_word in ACTION_VERBS\n",
    "\n",
    "\n",
    "def section_coverage_score(text: str) -> Tuple[float, Dict[str, bool]]:\n",
    "    found = {}\n",
    "    lower = text.lower()\n",
    "    for sec in EXPECTED_SECTIONS:\n",
    "        found[sec] = sec in lower\n",
    "    score = sum(found.values()) / len(EXPECTED_SECTIONS) if EXPECTED_SECTIONS else 0\n",
    "    return score, found\n",
    "\n",
    "\n",
    "def keyword_match_score(resume_text: str, jd_text: str) -> float:\n",
    "    resume_text = clean_text(resume_text)\n",
    "    jd_text = clean_text(jd_text)\n",
    "\n",
    "    if not jd_text or len(jd_text.split()) < 5:\n",
    "        return 0.0\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf = vectorizer.fit_transform([resume_text, jd_text])\n",
    "    sim = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "    return float(sim)\n",
    "\n",
    "\n",
    "def detect_weak_phrases(text: str) -> List[Dict]:\n",
    "    lower = text.lower()\n",
    "    findings = []\n",
    "    for phrase in WEAK_PHRASES:\n",
    "        start = 0\n",
    "        while True:\n",
    "            idx = lower.find(phrase, start)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            findings.append({\n",
    "                \"phrase\": phrase,\n",
    "                \"start\": idx,\n",
    "                \"end\": idx + len(phrase),\n",
    "            })\n",
    "            start = idx + len(phrase)\n",
    "    return findings\n",
    "\n",
    "\n",
    "def compute_ats_scores(resume_text: str, jd_text: str = \"\") -> Dict:\n",
    "    resume_text = clean_text(resume_text)\n",
    "\n",
    "    bullets = extract_bullets(resume_text)\n",
    "    sentences = split_into_sentences(resume_text)\n",
    "\n",
    "    # 1. Structure / sections\n",
    "    section_score, section_found = section_coverage_score(resume_text)\n",
    "\n",
    "    # 2. Keyword match vs JD (ML-ish part)\n",
    "    kw_score = keyword_match_score(resume_text, jd_text)  # 0‚Äì1\n",
    "\n",
    "    # 3. Action verbs\n",
    "    if bullets:\n",
    "        action_starts = sum(starts_with_action_verb(b) for b in bullets)\n",
    "        action_score = action_starts / len(bullets)\n",
    "    else:\n",
    "        action_score = 0.3  # neutral-ish default\n",
    "\n",
    "    # 4. Metrics in bullets\n",
    "    if bullets:\n",
    "        with_metrics = sum(contains_metric(b) for b in bullets)\n",
    "        metric_score = with_metrics / len(bullets)\n",
    "    else:\n",
    "        metric_score = 0.2\n",
    "\n",
    "    # 5. Length / readability\n",
    "    word_count = len(resume_text.split())\n",
    "    if word_count < 200:\n",
    "        length_score = 0.3\n",
    "    elif 200 <= word_count <= 800:\n",
    "        length_score = 1.0\n",
    "    elif 800 < word_count <= 1200:\n",
    "        length_score = 0.7\n",
    "    else:\n",
    "        length_score = 0.4\n",
    "\n",
    "    # Weighted ATS score (0‚Äì100)\n",
    "    final_score = (\n",
    "        section_score * 0.2 +\n",
    "        kw_score * 0.3 +\n",
    "        action_score * 0.2 +\n",
    "        metric_score * 0.15 +\n",
    "        length_score * 0.15\n",
    "    ) * 100\n",
    "\n",
    "    return {\n",
    "        \"final_score\": round(final_score, 1),\n",
    "        \"section_score\": round(section_score * 100, 1),\n",
    "        \"keyword_score\": round(kw_score * 100, 1),\n",
    "        \"action_score\": round(action_score * 100, 1),\n",
    "        \"metric_score\": round(metric_score * 100, 1),\n",
    "        \"length_score\": round(length_score * 100, 1),\n",
    "        \"word_count\": word_count,\n",
    "        \"bullets_count\": len(bullets),\n",
    "        \"section_found\": section_found,\n",
    "        \"sentences\": sentences,\n",
    "        \"bullets\": bullets,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_suggestions(analysis: Dict, weak_phrases: List[Dict], has_jd: bool) -> List[str]:\n",
    "    suggestions = []\n",
    "\n",
    "    # Sections\n",
    "    missing_sections = [sec for sec, present in analysis[\"section_found\"].items() if not present]\n",
    "    if missing_sections:\n",
    "        suggestions.append(\n",
    "            f\"Add or strengthen sections: **{', '.join(missing_sections)}** \"\n",
    "            f\"to match standard ATS expectations.\"\n",
    "        )\n",
    "\n",
    "    # Keyword match\n",
    "    if has_jd:\n",
    "        if analysis[\"keyword_score\"] < 50:\n",
    "            suggestions.append(\n",
    "                \"Your resume is not well-aligned with the job description. \"\n",
    "                \"Include more role-specific keywords and mirror terminology used in the JD.\"\n",
    "            )\n",
    "        else:\n",
    "            suggestions.append(\n",
    "                \"Good keyword alignment with the job description. Consider tailoring a few more bullet points \"\n",
    "                \"to emphasise the most important responsibilities.\"\n",
    "            )\n",
    "\n",
    "    # Action verbs\n",
    "    if analysis[\"action_score\"] < 60:\n",
    "        suggestions.append(\n",
    "            \"More bullet points should start with strong action verbs \"\n",
    "            f\"(e.g., {', '.join(ACTION_VERBS[:8])}, etc.).\"\n",
    "        )\n",
    "\n",
    "    # Metrics\n",
    "    if analysis[\"metric_score\"] < 40:\n",
    "        suggestions.append(\n",
    "            \"Quantify your impact: add numbers (%, $ or counts) to show scale (e.g., \"\n",
    "            \"\\\"Improved response time by 30%\\\", \\\"Managed a team of 5\\\").\"\n",
    "        )\n",
    "\n",
    "    # Length\n",
    "    if analysis[\"length_score\"] < 60:\n",
    "        if analysis[\"word_count\"] < 200:\n",
    "            suggestions.append(\n",
    "                \"The resume is quite short. Add more detail to your experience, skills, and projects.\"\n",
    "            )\n",
    "        elif analysis[\"word_count\"] > 1200:\n",
    "            suggestions.append(\n",
    "                \"The resume may be too long. Try to trim older or less relevant experience and keep it concise.\"\n",
    "            )\n",
    "\n",
    "    # Weak phrases\n",
    "    if weak_phrases:\n",
    "        unique_weak = sorted(set([w[\"phrase\"] for w in weak_phrases]))\n",
    "        suggestions.append(\n",
    "            \"Replace generic phrases like \"\n",
    "            + \", \".join(f\"**{p}**\" for p in unique_weak)\n",
    "            + \" with specific, impact-focused statements.\"\n",
    "        )\n",
    "\n",
    "    if not suggestions:\n",
    "        suggestions.append(\"Your resume is in good shape. Consider minor polishing for clarity and consistency.\")\n",
    "\n",
    "    return suggestions\n",
    "\n",
    "\n",
    "def highlight_text(text: str, weak_phrases: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Returns HTML with weak phrases wrapped in <mark>.\n",
    "    Assumes `weak_phrases` items have 'start' and 'end' indices.\n",
    "    \"\"\"\n",
    "    if not weak_phrases:\n",
    "        return text.replace(\"\\n\", \"<br>\")\n",
    "\n",
    "    # sort by start index\n",
    "    weak_phrases = sorted(weak_phrases, key=lambda x: x[\"start\"])\n",
    "\n",
    "    highlighted = []\n",
    "    last_idx = 0\n",
    "    for wp in weak_phrases:\n",
    "        start, end = wp[\"start\"], wp[\"end\"]\n",
    "        # add normal text before\n",
    "        highlighted.append(\n",
    "            escape_html(text[last_idx:start])\n",
    "        )\n",
    "        # add highlighted phrase\n",
    "        highlighted.append(\n",
    "            f\"<mark>{escape_html(text[start:end])}</mark>\"\n",
    "        )\n",
    "        last_idx = end\n",
    "\n",
    "    # remaining text\n",
    "    highlighted.append(escape_html(text[last_idx:]))\n",
    "\n",
    "    return \"\".join(highlighted).replace(\"\\n\", \"<br>\")\n",
    "\n",
    "\n",
    "def escape_html(text: str) -> str:\n",
    "    return (\n",
    "        text.replace(\"&\", \"&amp;\")\n",
    "            .replace(\"<\", \"&lt;\")\n",
    "            .replace(\">\", \"&gt;\")\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============\n",
    "# UI LAYOUT\n",
    "# ==============\n",
    "\n",
    "def main():\n",
    "    st.title(\"üìÑ ML-based ATS Resume Analyzer\")\n",
    "    st.write(\n",
    "        \"Upload your resume and (optionally) a job description. \"\n",
    "        \"This tool simulates an ATS-style screening and highlights weak phrases.\"\n",
    "    )\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"1. Upload Resume\")\n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"Resume file (.pdf, .docx, .txt)\",\n",
    "            type=[\"pdf\", \"docx\", \"txt\"],\n",
    "        )\n",
    "\n",
    "        st.header(\"2. Job Description (Optional)\")\n",
    "        jd_text = st.text_area(\n",
    "            \"Paste job description here to check keyword match\",\n",
    "            height=200,\n",
    "            placeholder=\"Paste the JD here for better ATS keyword analysis...\"\n",
    "        )\n",
    "\n",
    "        analyze_button = st.button(\"Analyze Resume üöÄ\")\n",
    "\n",
    "    if not analyze_button:\n",
    "        st.info(\"Upload your resume and click **Analyze Resume üöÄ** in the sidebar to start.\")\n",
    "        return\n",
    "\n",
    "    if not uploaded_file:\n",
    "        st.error(\"Please upload a resume file first.\")\n",
    "        return\n",
    "\n",
    "    resume_text = extract_text(uploaded_file)\n",
    "    if not resume_text or len(resume_text.strip()) < 20:\n",
    "        st.error(\"Could not extract enough text from the resume. Try another file or format.\")\n",
    "        return\n",
    "\n",
    "    has_jd = bool(jd_text and len(jd_text.strip().split()) > 5)\n",
    "\n",
    "    # ---- Run analysis ----\n",
    "    analysis = compute_ats_scores(resume_text, jd_text if has_jd else \"\")\n",
    "    weak_phrases = detect_weak_phrases(resume_text)\n",
    "    suggestions = generate_suggestions(analysis, weak_phrases, has_jd)\n",
    "\n",
    "    # ========== TOP SUMMARY ==========\n",
    "    st.subheader(\"Overall ATS Score\")\n",
    "\n",
    "    col1, col2, col3 = st.columns([2, 1, 1])\n",
    "\n",
    "    with col1:\n",
    "        st.metric(\"ATS Score\", f\"{analysis['final_score']}/100\")\n",
    "        st.write(\n",
    "            f\"**Word count:** {analysis['word_count']} | \"\n",
    "            f\"**Bullets:** {analysis['bullets_count']}\"\n",
    "        )\n",
    "\n",
    "        st.progress(min(analysis[\"final_score\"] / 100, 1.0))\n",
    "\n",
    "    with col2:\n",
    "        st.metric(\"Keyword Match\" + (\" (JD)\" if has_jd else \"\"), f\"{analysis['keyword_score']}%\")\n",
    "        st.metric(\"Sections Quality\", f\"{analysis['section_score']}%\")\n",
    "\n",
    "    with col3:\n",
    "        st.metric(\"Action Verbs\", f\"{analysis['action_score']}%\")\n",
    "        st.metric(\"Quantification\", f\"{analysis['metric_score']}%\")\n",
    "\n",
    "    # ========== DETAILED BREAKDOWN ==========\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Score Breakdown\")\n",
    "\n",
    "    colA, colB = st.columns(2)\n",
    "\n",
    "    with colA:\n",
    "        st.markdown(\"#### Sections Detected\")\n",
    "        for sec, present in analysis[\"section_found\"].items():\n",
    "            emoji = \"‚úÖ\" if present else \"‚ö†Ô∏è\"\n",
    "            st.write(f\"{emoji} {sec.title()}\")\n",
    "\n",
    "    with colB:\n",
    "        st.markdown(\"#### Length & Structure\")\n",
    "        st.write(f\"- **Word count:** {analysis['word_count']}\")\n",
    "        st.write(f\"- **Length score:** {analysis['length_score']}%\")\n",
    "        if analysis[\"bullets_count\"]:\n",
    "            st.write(f\"- **Bullet points:** {analysis['bullets_count']}\")\n",
    "        else:\n",
    "            st.write(\"- No bullet points detected (consider using bullets for experience).\")\n",
    "\n",
    "    # ========== WEAK PHRASES ==========\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Weak / Generic Phrases Detected\")\n",
    "\n",
    "    if not weak_phrases:\n",
    "        st.success(\"No common weak phrases detected ‚úÖ\")\n",
    "    else:\n",
    "        unique_weak = sorted(set(w[\"phrase\"] for w in weak_phrases))\n",
    "        st.warning(\n",
    "            \"These phrases are often considered vague or generic. \"\n",
    "            \"Try to replace them with specific, action-oriented achievements.\"\n",
    "        )\n",
    "        st.write(\", \".join(f\"**{p}**\" for p in unique_weak))\n",
    "\n",
    "    # ========== SUGGESTIONS ==========\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Improvement Suggestions\")\n",
    "\n",
    "    for s in suggestions:\n",
    "        st.markdown(f\"- {s}\")\n",
    "\n",
    "    # ========== RESUME TEXT WITH HIGHLIGHTS ==========\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Resume Text (Highlighted)\")\n",
    "\n",
    "    highlighted_html = highlight_text(resume_text, weak_phrases)\n",
    "    st.markdown(\n",
    "        f\"<div style='white-space: pre-wrap; font-family: monospace; font-size: 13px;'>{highlighted_html}</div>\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "\n",
    "    st.caption(\"Yellow highlights mark weak or generic phrases detected by the analyzer.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45939cac-3721-43b7-925e-47d8ac9012f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
